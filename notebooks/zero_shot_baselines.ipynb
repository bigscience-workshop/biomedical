{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/debo/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from utils.zero_shot_baselines import return_lm_baseline_zero_shot, nli_based_zero_shot\n",
    "\n",
    "dataset_l = load_dataset(\"pubmed_qa\", \"pqa_labeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', '.', \"'\", ':', ')', ']', '(', '!', '-', '•', '/', '*', '[', '·', '1']\n",
      "['?', '.', '!', ':', \"'\", ']', ')', '(', '-', ';', '•', '[', '*', '>', 'a']\n",
      "['.', '?', \"'\", '!', ':', ')', ']', '(', '`', '-', '/', '[', '¿', '1', ';']\n",
      "['?', '.', \"'\", '!', ':', ')', ']', '(', '}', '[', '¿', '-', '#', '*', '/']\n",
      "['.', '?', \"'\", ':', ')', '!', ']', '(', ',', '*', '[', '¿', '-', '}', ';']\n",
      "['?', '.', \"'\", '!', ':', ']', ')', '(', '}', '*', '/', '[', '•', '>', '-']\n",
      "['?', '.', \"'\", '!', ':', ']', ')', '(', '*', '}', '-', 'to', '>', '[', '#']\n",
      "['?', '.', \"'\", ':', ')', ']', '!', '(', '-', '[', '¿', '`', '·', '*', '/']\n",
      "['?', '.', \"'\", '!', ']', ')', ':', '(', '}', '¿', '-', '[', '`', 'do', ',']\n",
      "['?', '.', \"'\", ':', ')', '!', ']', '(', '/', '[', '*', '-', '}', '`', 'to']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModel\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "for k in range(10):\n",
    "    sample = dataset_l['train'][k]\n",
    "    premise = ','.join(sample['context']['contexts'])  \n",
    "    sequence = f\"{premise} {sample['question']}? Answer:  {tokenizer.mask_token}\"\n",
    "    res = return_lm_baseline_zero_shot(model, tokenizer, sequence)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d7d028397f4f87bc0ea056aaf2b5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 59.0\n"
     ]
    }
   ],
   "source": [
    "ctr = 0\n",
    "num_items=100\n",
    "for k in tqdm(range(num_items)):\n",
    "    sample = dataset_l['train'][k]\n",
    "    # pose sequence as a NLI premise and label answer as a hypothesis\n",
    "    premise = ','.join(sample['context']['contexts']) + sample['question']\n",
    "    # premise = f\"Given the context {premise}\"\n",
    "    hypothesis = sample['long_answer']\n",
    "    res = nli_based_zero_shot(model, tokenizer, premise, hypothesis)\n",
    "    out = 'yes' if res>0.5 else 'no'\n",
    "    ctr+= out==sample['final_decision']\n",
    "print(f\"Accuracy : {ctr/num_items*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
