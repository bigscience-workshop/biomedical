{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, interleave_datasets, logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel, AutoModelWithLMHead, AutoModel, AutoTokenizer\n",
    "from fewshot import truncate_text_for_prompt, evaluate, tokenizer_and_numericalize, get_accuracy\n",
    "from prompt_model import TransformerForPrompting\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"boolq\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\") \n",
    "\n",
    "dt = 2000\n",
    "\n",
    "for_prompt_training = dataset['train']\n",
    "for_prompt_testing = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2995, 6270, 2748, 2053)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('true'), tokenizer.convert_tokens_to_ids('false'), tokenizer.convert_tokens_to_ids('yes'), tokenizer.convert_tokens_to_ids('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 1.13 s, total: 19.6 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds = for_prompt_training.map(tokenizer_and_numericalize, fn_kwargs={'truncate_text_for_prompt': truncate_text_for_prompt, 'tokenizer': tokenizer}, num_proc=32)\n",
    "train_ds.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "\n",
    "test_ds = for_prompt_testing.map(tokenizer_and_numericalize, fn_kwargs={'truncate_text_for_prompt': truncate_text_for_prompt, 'tokenizer': tokenizer}, num_proc=32)\n",
    "test_ds.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "output_dim = len(set(dataset['train']['answer']))\n",
    "transformer = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model = TransformerForPrompting(transformer, output_dim=output_dim, freeze=False)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "model = model.to(device)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  45%|████▍     | 530/1179 [04:09<05:06,  2.12it/s, accuracy=0.629, task_loss=0.659]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bd5bedea7e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# calculate loss for every parameter that needs grad update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fewshot/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fewshot/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "model.train()\n",
    "\n",
    "acc_total = []\n",
    "lm_loss_total = []\n",
    "tl_total = []\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        task_labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        task_loss = loss_fn(outputs[0], task_labels)\n",
    "        \n",
    "        loss = task_loss\n",
    "        accuracy = get_accuracy(outputs[0], task_labels)\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        \n",
    "        acc_total.append(accuracy.item())\n",
    "        tl_total.append(task_loss.item())\n",
    "#         print(acc_total, lm_loss_total, tl_total)\n",
    "        \n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(task_loss=np.mean(tl_total), accuracy = np.mean(acc_total))\n",
    "        \n",
    "    epoch_losses_tl, epoch_accs = evaluate(test_loader, model, loss_fn, device)\n",
    "    print(f\"Evaluation Accuracy: {np.mean(epoch_accs)}  Evaluation loss Task: {np.mean(epoch_losses_tl)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'/hd2/prompting_models/imdb/fine_tune_with_correct_model_{dt}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'/hd2/prompting_models/imdb/transformer_with_correct_model_{dt}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_losses_lm, epoch_losses_tl, epoch_accs = evaluate(test_loader, model, loss_fn, device)\n",
    "# np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pattern_verbalizer1(question, passage, answer, for_eval=False):\n",
    "#     return f\"Based on the following passage: {passage} {question}. {tokenizer.mask_token}\"\n",
    "    \n",
    "# def pattern_verbalizer2(question, passage, answer, for_eval=False):\n",
    "#     return f\"From the following passage: {passage} {question}. {tokenizer.mask_token}\" \n",
    "    \n",
    "# def pattern_verbalizer3(question, passage, answer, for_eval=False):\n",
    "#     return f\"{passage} Question: {question}. Answer: {tokenizer.mask_token}\" \n",
    "    \n",
    "# def pattern_verbalizer4(question, passage, answer, for_eval=False):\n",
    "#     return f\"Question: {question}? Passage: {passage} Answer: {tokenizer.mask_token}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_votes_for_pattern(ds = dataset['validation'], pattern_function=pattern_verbalizer1):\n",
    "#     val1 = ds.map(tokenizer_and_numericalize, fn_kwargs={'pattern_func': pattern_function, 'truncate_text_for_prompt': truncate_text_for_prompt}, num_proc=32)\n",
    "#     val1.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "#     val_data = val1.map(create_lm_labels, fn_kwargs={'subset': 'test'}, num_proc=32)\n",
    "#     val_data.set_format(type='torch', columns=['label', 'input_ids', 'token_type_ids', 'attention_mask', 'lm_labels'], device=device)\n",
    "#     val_loader = torch.utils.data.DataLoader(val_data, batch_size=16, shuffle=True)\n",
    "#     res = []\n",
    "#     dict_reverse = {'false': False, 'true': True, 'no': False, 'yes': True}\n",
    "#     for example in tqdm(val_data):\n",
    "#         with torch.no_grad():\n",
    "#             input_ids = example['input_ids'].unsqueeze(0).to(device)\n",
    "#             attn = example['attention_mask'].unsqueeze(0).to(device)\n",
    "#             mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "#             token_logits = model(input_ids, attn)[1]\n",
    "#             mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "#             res.append((bool(example['label'].item()), dict_reverse[tokenizer.decode([mask_token_logits.squeeze().argmax()])]))\n",
    "#     accuracy = sum(int(a == b) for a,b in res)/len(res)\n",
    "#     return res, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res1, accuracy1 = get_votes_for_pattern(pattern_function=pattern_verbalizer1)\n",
    "# res2, accuracy2 = get_votes_for_pattern(pattern_function=pattern_verbalizer2)\n",
    "# res3, accuracy3 = get_votes_for_pattern(pattern_function=pattern_verbalizer3)\n",
    "# res4, accuracy4 = get_votes_for_pattern(pattern_function=pattern_verbalizer4)\n",
    "# accuracy1, accuracy2, accuracy3, accuracy4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl1 = [j for i, j in res1]\n",
    "# cl2 = [j for i, j in res2]\n",
    "# cl3 = [j for i, j in res3]\n",
    "# cl4 = [j for i, j in res4]\n",
    "# truth = [i for i, j in res1]\n",
    "\n",
    "# result = pd.DataFrame({'truth': truth, 'pattern1': cl1, 'pattern2': cl2, 'pattern3': cl3, 'pattern4': cl4 })\n",
    "# result['majority'] = result.apply(lambda x: Counter([x['pattern1'], x['pattern2'], x['pattern3'], x['pattern4']]).most_common(1)[0][0],axis=1)\n",
    "# acc = sum(result['truth']==result['majority'])/len(result)\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
